{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Datastore based on KG for MIMIC-NLE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "from medclip import MedCLIPProcessor\n",
    "import torch\n",
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"We load in all images and only the train captions.\"\"\"\n",
    "\n",
    "    annotations = json.load(open(data_path))\n",
    "    images = []\n",
    "    captions = []\n",
    "    for item in annotations:\n",
    "        if item['split'] == 'train':\n",
    "            if 'triplets' in item:\n",
    "                captions.append({'image_id': item['img_id'],  'caption': item['triplets']})\n",
    "        images.append({'image_id': item['img_id'], 'img_path': item['img_path']})\n",
    " \n",
    "    return images, captions\n",
    "\n",
    "def encode_captions(captions, model, device, preprocess):\n",
    "    bs = 256\n",
    "    encoded_captions = []\n",
    "    model.to(device)\n",
    "\n",
    "    for idx in tqdm(range(0, len(captions), bs)):\n",
    "        batch_captions = captions[idx:idx+bs]\n",
    "        \n",
    "        # Tokenize the captions with truncation and padding\n",
    "        input_ids = preprocess(text=batch_captions, return_tensors='pt', padding=True, truncation=True, max_length=77).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded_batch = model.encode_text(input_ids['input_ids']).cpu().numpy()\n",
    "            encoded_captions.append(encoded_batch)\n",
    "\n",
    "    encoded_captions = np.concatenate(encoded_captions) if encoded_captions else np.array([])\n",
    "\n",
    "    return encoded_captions\n",
    "\n",
    "def encode_images(images, data_dir, model, feature_extractor, device):\n",
    "    image_ids = [i['image_id'] for i in images]\n",
    "    \n",
    "    bs = 64\t\n",
    "    image_features = []\n",
    "    for idx in tqdm(range(0, len(images), bs)):\n",
    "        batch_images = images[idx:idx + bs]  # Get a batch of image data\n",
    "        img_paths = [img['img_path'] for img in batch_images]  # Extract img_path from each image data\n",
    "        processed_images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\n",
    "        image_input = feature_extractor(images=processed_images, return_tensors='pt').pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embeds = model.encode_image(pixel_values=image_input).cpu().numpy()\n",
    "            image_features.append(image_embeds)\n",
    "    image_features = np.concatenate(image_features)\n",
    "\n",
    "    return image_ids, image_features\n",
    "\n",
    "def filter_captions(data):\n",
    "\n",
    "    decoder_name = 'gpt2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(decoder_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    bs = 512\n",
    "\n",
    "    image_ids = [d['image_id'] for d in data]\n",
    "    caps = [d['caption'] for d in data]\n",
    "    encodings = []\n",
    "    for idx in range(0, len(data), bs):\n",
    "        encodings += tokenizer.batch_encode_plus(caps[idx:idx+bs], return_tensors='np',padding=True, truncation=True, max_length=100 )['input_ids'].tolist()\n",
    "    \n",
    "    filtered_image_ids, filtered_captions = [], []\n",
    "\n",
    "    assert len(image_ids) == len(caps) and len(caps) == len(encodings)\n",
    "    for image_id, cap, encoding in zip(image_ids, caps, encodings):\n",
    "        if len(encoding) <= 100:\n",
    "            filtered_image_ids.append(image_id)\n",
    "            filtered_captions.append(cap)\n",
    "\n",
    "    return filtered_image_ids, filtered_captions\n",
    "\n",
    "\n",
    "def get_nns(captions, images, k=15):\n",
    "    xq = images.astype(np.float32)\n",
    "    xb = captions.astype(np.float32)\n",
    "    faiss.normalize_L2(xb)\n",
    "    index = faiss.IndexFlatIP(xb.shape[1])\n",
    "    index.add(xb)\n",
    "    faiss.normalize_L2(xq)\n",
    "    D, I = index.search(xq, k) \n",
    "\n",
    "    return index, I\n",
    "\n",
    "def filter_nns(nns, xb_image_ids, captions, xq_image_ids):\n",
    "    \"\"\"We filter out nearest neighbors which are actual captions for the query image, keeping 7 neighbors per image.\"\"\"\n",
    "    retrieved_captions = {}\n",
    "    for nns_list, image_id in zip(nns, xq_image_ids):\n",
    "        good_nns = []\n",
    "        for nn in nns_list:  # Iterate directly over nns_list\n",
    "            if xb_image_ids[nn] == image_id:\n",
    "                continue\n",
    "            good_nns.append(captions[nn])\n",
    "            if len(good_nns) == 7:\n",
    "                break\n",
    "        assert len(good_nns) == 7\n",
    "        retrieved_captions[image_id] = good_nns\n",
    "    return retrieved_captions\n",
    "\n",
    "def main(): \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder_name = MedCLIPVisionModelViT\n",
    "    feature_extractor = MedCLIPProcessor()\n",
    "    clip_model = MedCLIPModel(vision_cls=encoder_name)\n",
    "    clip_model.from_pretrained()\n",
    "    clip_model = clip_model.cuda()\n",
    "\n",
    "    # load captions from json file\n",
    "    annotations = json.load(open(\"/data/mimic_nle_kg_suggestive.json\", \"r\"))\n",
    "    captions = []\n",
    "    for item in annotations:\n",
    "        if item['split'] == 'train':\n",
    "            if 'triplets' in item:\n",
    "                captions.append({'image_id': item['img_id'],  'caption': item['triplets']})\n",
    "\n",
    "    print('Filtering captions')    \n",
    "    xb_image_ids, captions = filter_captions(captions)\n",
    "\n",
    "    print('Encoding captions')\n",
    "    encoded_captions = encode_captions(captions, clip_model, device,preprocess=feature_extractor)\n",
    "\n",
    "    print('Encoding images')\n",
    "    xq_image_ids, encoded_images = encode_images(images, image_path, clip_model, feature_extractor, device)\n",
    "\n",
    "    print('Retrieving neighbors')\n",
    "    index, nns = get_nns(encoded_captions, encoded_images)\n",
    "    retrieved_caps = filter_nns(nns, xb_image_ids, captions, xq_image_ids)\n",
    "\n",
    "    print('Writing files')\n",
    "    faiss.write_index(index, \"/data/datastore/kg_nle_index\")\n",
    "    json.dump(captions, open('/data/datastore/kg_nle_index_captions.json', 'w'))\n",
    "\n",
    "    json.dump(retrieved_caps, open('/data/retrieved_triplets.json', 'w'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset for LLaVA Training on MIMIC-NLE dataset with KG-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the dataset in LLaVA format.\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Input JSON file paths\n",
    "input_json_file = '/data/mimic_nle_dataset.json'\n",
    "kg_triplets_file = '/data/retrieved_triplets.json'\n",
    "\n",
    "# Output JSON file path\n",
    "output_json_file = '/data/mimic-nle-train-.json'\n",
    "\n",
    "# Number of KG triplets to pick\n",
    "K = 7\n",
    "\n",
    "# Seed value for reproducibility\n",
    "seed_value = 42\n",
    "\n",
    "# Diagnosis list and certainty list as provided\n",
    "diagnosis_list = [\n",
    "    \"Atelectasis\",\n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\",\n",
    "    \"Lung Lesion\",\n",
    "    \"Lung Opacity\",\n",
    "    \"Pleural Effusion\",\n",
    "    \"Pleural Other\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]\n",
    "certainty_list = [\"negative\", \"uncertain\", \"positive\"]\n",
    "\n",
    "# Question templates\n",
    "question_templates = [\n",
    "    \"Which signs show that the patient has {pathologies}?\",\n",
    "    \"Explain why these {pathologies} are present in the image.\",\n",
    "    \"What evidence in the image indicates {pathologies}?\",\n",
    "    \"How can you tell that the patient has {pathologies} from the image?\",\n",
    "    \"What features suggest the presence of {pathologies} in this image?\"\n",
    "]\n",
    "\n",
    "# Function to get pathologies string from record\n",
    "def get_pathologies(record):\n",
    "    prompt = \"\"\n",
    "    for idx, diagnosis in enumerate(record['img_labels']):\n",
    "        if diagnosis[1]:\n",
    "            prompt += certainty_list[1] + \" \" + diagnosis_list[idx] + \", \"\n",
    "        if diagnosis[2]:\n",
    "            prompt += certainty_list[2] + \" \" + diagnosis_list[idx] + \", \"\n",
    "    return prompt.strip(\", \")\n",
    "\n",
    "# Function to load data and KG triplets\n",
    "def load_data_with_kg_triplets(annot_path, triplets_path, K):\n",
    "    annotations = json.load(open(annot_path))\n",
    "    kg_triplets = json.load(open(triplets_path))\n",
    "    data = {}\n",
    "    \n",
    "    for item in annotations:\n",
    "        triplets = kg_triplets.get(str(item['img_id']), [])\n",
    "        item['kg_triplets'] = triplets[:K]  # Pick the first K triplets\n",
    "        data[str(item['img_id'])] = item\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Load annotations and KG triplets\n",
    "data = load_data_with_kg_triplets(input_json_file, kg_triplets_file, K)\n",
    "\n",
    "# Prepare the output data\n",
    "output_data = []\n",
    "for record_id, record in data.items():\n",
    "    if record[\"split\"] == \"train\" or record[\"split\"] == \"val\":\n",
    "        pathologies = get_pathologies(record)\n",
    "        question_template = random.choice(question_templates)\n",
    "        question = question_template.format(pathologies=pathologies)\n",
    "        kg_triplets = \"; \".join(record.get('kg_triplets', []))\n",
    "        \n",
    "        conversation = {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": f\"<image>\\nThe image-specific triplets from the knowledge graph are: {kg_triplets}. And for the given image, {question}\"\n",
    "            # for test set\n",
    "            #\"value\": f\"The image-specific triplets from the knowledge graph are: {kg_triplets}. And for the given image, {question}\"\n",
    "        }\n",
    "        assistant_response = {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": record[\"nle\"]\n",
    "        }\n",
    "        output_record = {\n",
    "            \"id\": str(record[\"img_id\"]),\n",
    "            \"split\": record[\"split\"],\n",
    "            \"image\": record[\"img_path\"],\n",
    "            \"conversations\": [conversation, assistant_response]\n",
    "        }\n",
    "        output_data.append(output_record)\n",
    "\n",
    "# Write the output JSON file\n",
    "with open(output_json_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(f\"Output JSON file has been created successfully: {output_json_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
